# 图解操作系统

## 一、硬件结构

### 1.1 CPU是如何执行程序的的？

代码写了这么多，你知道 ``a=1+2``这条代码是怎么被cpu执行的吗？

你知道32位和64位的区别吗？32位的操作系统可以运行在64位的电脑上吗？64位的操作系统可以运行在32位的电脑上吗？如果不行，原因是什么？

cpu看了那么多，我们知道cpu通常分为32位和64位，你知道64位相比32位cpu的优势在哪吗？64位cpu的计算性能一定比32位cpu高很多吗？

接下来我们将循序渐进、一层一层的攻破这些问题。

#### 图灵机的工作方式

要想知道程序执行的原理，我们可以先从 ``图灵机``说起，图灵的基本思想是**用机器来模拟人们用纸笔进行数学运算的过程**，而且还定义了计算机有哪些部分组成，程序又是如何执行的。

图灵机的基本组成如下：

- 有一条 ``纸带``,纸带是一个个连续的格子组成，每个格子可以写入字符，纸带就好比内存，而纸带上的格子的字符就好比内存中的数据或程序；
- 有一个 ``读写头``,读写头可以读取纸带上任意格子的字符，也可以把字符写入到纸带的格子;
- 读写头上有一些部件，比如存储单元、控制单元以及运算单元：
- 1. 存储单元用于存放数据
- 2. 控制单元用于识别字符是数据还是指令，以及控制程序的流程等；
- 3. 运算单元用于执行运算指令；

知道了图灵机的组成后，我们以简单数学运算 ``1+2``作为例子，来看看它是怎么执行这行代码的。

- 首先，用读写头把‘1’、‘2’、‘+’这三个字符分别写入纸带上的三个格子，然后读写头先停在 ``1``字符对应的格子上；
- 接着，读写头读入 ``1``到存储设备上，这个存储设备称为图灵机的状态；
- 然后读写头向右移动一个格，用同样的方式把 ``2``读入图灵机的状态，于是现在图灵机的状态中存储着两个连续的数字，1和2；
- 读写头再往右移动一个格，就会碰到 ``+``号，读写头读到+号后，将+号传输给 ``控制单元``，控制单元发现是一个+号而不是数字，所以没有存入到状态中，因为+号是运算符指令，作用是加和目前的状态，于是通知 ``运算单元``工作。运算单元收到要加和状态中的值的通知后，就会把状态中的1和2读入并计算，再将计算的结果3存放到状态中；
- 最后，运算单元将结果返回给控制单元，控制单元将结果传输给读写头，读写头向右移动，把结果3写入到纸带的格子中；

通过上面的图灵机计算 ``1+2``的过程，可以发现图灵机主要功能就是读取纸带格子中的内容，然后交给控制单元识别字符是数字还是运算符指令，如果是数字则存入到图灵机状态中，如果是运算符，则通知运算符单元读取状态中的数值进行计算，计算结果最终返回给读写头，读写头把结果写入到纸带的格子中。

事实上，图灵机这个看起来很简单的工作方式，和我们今天的计算机时基本一样的。接下来，我们一同再看看当今计算机的组成以及工作方式。

---

#### 冯诺依曼模型

在1945年冯诺依曼和其他计算机科学家们提出了计算机具体实现的报告，其遵循了图灵机的设计，而且还提出用电子元件构造计算机，并约定了用二进制进行计算和存储，还定义计算机基本结构为5个部分，分别是：

- 中央处理器CPU
- 内存
- 输入设备
- 输出设备
- 总线

这5个部分也被称为冯诺依曼模型，接下来看这5部分的具体作用:

##### 内存

我们的程序和数据都是存储在内存，存储的区域是线性的。

数据存储的单位是一个**二进制位（bit）**，即0或1.最小的存储单位是**字节（byte）**，1字节等于8位。

内存的地址是从0开始编号的，然后自增排列，最后一个地址位内存总字节数-1，这种结构好似我们程序里的数组，所以内存的读写任何一个数据的速度都是一样的。

##### 中央处理器

中央处理器也就是我们常说的CPU，32位和64位CPU最主要区别在于一次能计算多少字节数据：

- 32位cpu一次可以计算4个字节；
- 64位cpu一次可以计算8个字节。

这里的32位和64位，通常称为CPU的位宽。

之所以CPU要这样设计，是为了能计算更大的数值，如果是8位的CPU，那么一次只能计算1个字节 ``0~255``范围内的数值，这样就无法一次完成计算 ``10000*500``，于是为了能一次计算大数的运算，CPU需要支持多个byte一起计算，所以CPU位宽越大，可以计算的数值就越大，比如说32位CPU能计算的最大整数是 ``4294967295``。

CPU内部还有一些组件，常见的有寄存器、控制单元和逻辑运算单元等。其中，控制单元负责控制CPU工作，逻辑运算单元负责计算，而寄存器可以分为多种类，每种寄存器的功能又不尽相同。

CPU中的寄存器主要作用是存储计算时的数据，你可能会好奇为什么有了内存还需要寄存器？原因很简单，因为内存离cpu太远了，而寄存器就在cpu里，还紧挨着控制单元和逻辑运算单元，自然计算时速度会很快。

常见的寄存器种类：

- **通用寄存器**：用来存放需要进行运算的数据，比如需要进行加和运算的两个数据。
- **程序计数器**：用来存储cpu要执行下一条指令“所在的内存地址”，注意不是存储了下一条要执行的指令，此时指令还在内存中，程序计数器只是存储了下一条指令的地址。
- **指令寄存器**：用来存放程序计数器指向的指令，也就是指令本身，指令被执行完成之前，指令都存储在这里。

##### 总线

总线是用于cpu和内存以及其他设备之间的通信，总线可分为3种：

- **地址总线**：用于指定cpu要操作的内存地址；
- **数据总线**：用于读写内存的数据；
- **控制总线**：用于发送和接收信号，比如中断、设备复位等信号，cpu收到信号后自然进行响应，这时也需要控制总线。

当cpu要读写内存数据的时候，一般需要通过两个总线：

- 首先要通过 ``地址总线``来指定内存的地址；
- 再通过 ``数据总线``来传输数据。

##### 输入、输出设备

输入设备向计算机输入数据，计算机经过计算后，把数据输出给输出设备。期间，如果输入设备是键盘，按下按键时是需要和cpu进行交互的，这时就需要用到控制总线了。

#### 线路位宽与CPU位宽

数据是如何通过线路传输的呢？其实是通过操作电压，低电压表示0、高压电压则表示1。

如果构造了 ``高低高``这样的信号，其实就是 ``101``二进制数据，十进制则表示5，如果只有一条线路，就意味着每次只能传递1 bit的数据，即0或1，那么传输101这个数据，就需要3此才能传输完成，这样的效率非常低。

这样一位一位传输的方式，称为串行，下一个bit必须等待上一个bit传输完成才能进行传输。当然，想一次多传一些数据，增加线路即可，这时数据就可以并行传输。

为了避免低效率的串行传输的方式，线路的位宽最好一次就能访问到所有的内存地址。cpu要想操作的内存地址就需要地址总线，如果地址总线只有一条，那每次只能表示 ``0或1``这两种情况，所以cpu一次只能操作2个内存地址，如果想要cpu操作4G的内存，那么就需要32条地址总线，因为 ``2^32=4G``。

知道了线路位宽的意义后，我们再来看看cpu位宽。

cpu的位宽最好不要小于线路位宽，比如32位cpu控制40位宽的地址总线和数据总线的话，工作起来就会非常复杂且麻烦，所以32位的cpu最好和32位宽的线路搭配，因为32位cpu一次最多只能操作32位宽的地址总线和数据总线。

如果用32位cpu去加和两个64位大小的数字，就需要把这2个64位的数字分成2个低位32位数字和2个高位32位数字来计算，先加和两个低位的32位数字，算出进位，然后加和两个高位的32位数字，最后再加上进位，救恩那个算出结果了，可以发现32位cpu并不能一次性计算出加和两个64位数字的结果。

但是并不代表64位cpu性能比32位cpu高很多，很少应用需要算超过32位的数字，所以
``如果计算的数额不超过32位数字的情况下，32位和64位cpu之间没什么区别的，只有当计算超过32位数字的情况下，64位的优势才能体现出来。``

另外，32位cpu最大只能操作4GB内存，就算装了8 GB内存条，也没用。而64位CPU寻址范围则很大，理论最大的寻址空间位 ``2^64``.

#### 程序执行的基本过程

在前面，我们知道了程序在图灵机的执行过程，接下来我们看看程序在冯诺依曼模型上是怎么执行的。

程序执行实际上是一条一条指令，所以程序的运行过程就是把每一条指令一步一步的执行起来，负责执行指令的就是cpu了。

那cpu执行程序的过程如下：

- 第一步：cpu读取 ``程序计数器``的值，这个值是指令的内存地址，然后cpu的 ``控制单元``操作 ``地址总线``指定需要访问的内存地址，接着通知内存设备准备数据，数据准备好后通过 ``数据总线``将指令数据传给cpu，cpu收到内存传来的数据后，将这个指令数据存入到 ``指令寄存器``。
- 第二步：cpu分析 ``指令寄存器``中的指令，确定指令的类型和参数，如果是计算类型的指令，则交由 ``控制单元``执行；
- 第三步：cpu执行完指令后，``程序计数器``的值自增，表示指向下一条指令。这个自增的大小，由cpu的位宽决定，比如32位的cpu，指令是4个字节，需要4个内存地址存放，因此 ``程序计数器``的值会自增4。

简单总结一下就是：
``一个程序执行的时候，cpu会根据程序计数器里的内存地址，从内存里把需要执行的指令读取到指令寄存器里面执行，然后根据指令长度自增，开始顺序读取下一条指令。``

cpu从程序计数器读取指令、到执行、再到下一条指令，这个过程会不断循环，直到程序执行结束，这个不断循环的过程被称为**cpu的指令周期**。

#### a = 1+2 执行具体过程

知道了基本的程序执行过程后，接下来用 ``a=1+2``的作为例子，进一步分析该程序在冯诺依曼模型的执行过程。

cpu是不认识a=1+2这个字符串，这些字符串只是方便我们程序员认识，要想这段程序能跑起来，还需要把整个程序翻译成 ``汇编``程序，这个过程称为编译成汇编代码。

针对汇编代码，我们还需要用汇编器翻译成 ``机器码``，这些机器码由0到1组成的机器语言，这一条条机器码，就是一条条的 ``计算机指令``，这个才是cpu能够真正认识的东西。

下面来看看a=1+2在32位cpu的执行过程。

程序编译过程中，编译器通过分析代码，发现1和2是数据，于是程序运行时，内存会有个专门的区域来存放这些数据，这个区域就是 ``数据段``。

- 数据 1 被存放到 0x100 位置
- 数据 2 被存放到 0x104 位置

注意，数据和指令是分开区域存放的，存放指令区域的地方称为 ``正文段``。

编译器会把a=1+2翻译成4条指令，存放到正文段中：

- ``0x200 load 0x100->R0`` : 0x200的内容是 ``load``指令将0x100地址中的数据1装入到寄存器 ``R0``;
- ``0x204 load 0x104->R1`` : 0x204的内容是 ``load``指令将0x104地址中的数据2装入到寄存器 ``R1``；
- ``0x208 add R0 R1 R2`` : 0x208的内容是 ``add``指令将寄存器R0和R1的数据相加，并把结果存放到寄存器R2；
- ``0x20c set R2->0x108`` : 0x20c的内容是 ``store``指令将寄存器R2中的数据存回数据段中的0x108地址中，这个地址也就是变量 ``a``内存中的地址。

编译完成后，具体执行程序的时候，程序计数器会被设置为0x200地址，然后依次执行这4条指令。

上面的例子中，由于是在32位cpu执行的，因此一条指令是占32位大小，所以你会发现每条指令间隔4个字节。

而数据的大小是根据你在程序中指定的变量类型，比如 ``int``类型的数据则占4个字节，``char``类型的数据则占1个字节。

##### 指令

上面的例子中，指令的内容是写的简易的汇编diamagnetic，目的是为了方便理解指令的具体内容，事实上指令的内容是一串二进制数字的机器码，每条指令都有对应的机器码，cpu通过解析机器码来知道指令的内容。

不同的cpu有不同的指令集，也就是对应着不同的汇编语言和不同的机器码，接下来选用最简单的MIPS指令集，来看看机器码是如何生成的，这样也能明白二进制的机器码的具体含义。

MIPS的指令是一个32位的整数，高6位代表着操作码，表示这条指令是一条什么样的指令，剩下的26位不同指令类型所表示的内容也就不相同，主要有三种类型R、I、J。

| 指令类型 | 6位              | 5位 | 5位 | 5位                              | 5位             | 6位                        |
| -------- | ---------------- | --- | --- | -------------------------------- | --------------- | -------------------------- |
| R        | opcode（操作码） | rs  | rt  | rd                               | shamt（位移量） | funct（功能码）            |
| I        | opcode（操作码） | rs  | rt  | address/immediate（地址/立即数） |                 |                            |
| J        | opcode（操作码） |     |     |                                  |                 | target address（目标地址） |

三种类型的具体含义：

- **R指令**：用在算数和逻辑操作，里面由读取和写入数据的寄存器地址。如果是逻辑位移操作，后面还有位移操作的 ``位移量``，而最后的 ``功能码``则是前面的操作码不够的时候，扩展操作码来表示对应的具体指令的；
- **I指令**：用在数据传输、条件分支等。这个类型的指令，就没有了位移码和操作码，也没有了第三个寄存器，而把这三部分直接合并成了一个地址值或一个常数；
- **J指令**：用在跳转，高6位之外的26位都是一个跳转后的地址。

接下来，我们把前面例子的这条指令：``add``指令将寄存器 ``R0``和 `R1`的数据相加，并把结果放入到 `R2`，翻译成机器码。

| 指令         | 指令类型 | 操作码 6位 | rs 5位 | rt 5位 | rd 5位 | 位移量 5位 | 功能码/十六进制 |
| ------------ | -------- | ---------- | ------ | ------ | ------ | ---------- | --------------- |
| add          | R        | 000000     | 00000  | 00001  | 00010  | 00000      | 100000（funct） |
| 十六进制表示 |          |            |        |        |        |            | 0x00011020      |

加和运算add指令是属于R指令类型：

- add对应的MIPS指令里操作码是 `000000`，以及最末尾的功能码是 `100000`，这些数值都是固定的，查一下MIPS指令集的手册就能知道的；
- rs代表第一个寄存器R0的编号，即 `00000`；
- rt代表第二个寄存器R1的编号，即 `00001`；
- rd代表目标的临时寄存器R2的编号，即 `00010`；
- 因为不是位移操作，所以位移量是 `00000`

把上面这些数字拼在一起就是一条32位的MIPS加法指令了，那么用16进制表示的机器码则是 `0x00011020`。

编译器在编译程序的时候，会构造指令，这个过程叫作指令的编码。cpu执行程序的时候，就会解析指令，这个过程叫作指令的解码。

现代大多数cpu都使用来流水线的方式来执行指令，所谓的流水线就是把一个任务拆分成多个小任务，于是一条指令通常分为4个阶段，称为四级流水线

```mermaid
flowchart LR
    A[<b>① Fetch</b>]:::fetchStyle --> B[<b>② Decode</b>]:::decodeStyle
    B --> C[<b>③ Execute</b>]:::executeStyle
    C --> D[<b>④ Store</b>]:::storeStyle
    D --> A
```

四个阶段的具体含义：

1. CPU通过程序计数器读取对应内存地址的指令，这个部分称为 `Fetch（取得指令）`；
2. CPU对指令进行解码，这个部分称为 `Decode（指令译码）`；
3. CPU执行指令，这个部分称为`Execution（执行指令）`;
4. CPU将计算结果存回寄存器或者将寄存器的值存入内存，这个部分称为`Stroe（数据回写）`。

上面这四个阶段，我们称为`指令周期（Instruction Cycle）`，CPU的工作就是一个周期接着一个周期，周而复始。

事实上，不同的阶段其实是由计算机中的不同组件完成的：
- 取指令阶段，我们的指令是存放在`存储器`里的，实际上，通过程序计数器和指令寄存器取出指令的过程，是由`控制器`操作的；
- 指令的译码过程，也是由`控制器`进行的；
- 指令执行的过程，无论是进行算术操作、逻辑操作，还是进行数据传输、条件分支操作，都是由`算术逻辑单元`操作的，也就是由`运算器`处理的，但是如果是一个简单的无条件地址跳转，则是直接在`控制器`里面完成的，不需要用到运算器。

##### 指令类型

指令从功能角度划分，可以分为5大类：
- **数据传输类型的指令**：比如`store/load`是寄存器与内存间数据传输的指令，`mov`是将一个内存地址的数据移动到另一个内存地址的指令；
- **运算类型的指令**：比如加减乘除、位运算、比较大小等等，它们最多只能处理两个寄存器中的数据；
- **跳转类型的指令**：通过修改程序计数器的值来达到跳转执行指令的过程，比如编程中常见的`if-else`、`swtich-case`、函数调用等。
- **信号类型的指令**：比如发生中断的指令`trap`；
- **闲置类型的指令**：比如指令`nop`，执行后cpu会空转一个周期。

##### 指令的执行速度

CPU的硬件参数都会有`GHz`这个参数，比如一个1GHz的cpu，指的是时钟频率是1 G，代表着一秒钟会产生1G次数的脉冲信号，每一次脉冲信号高低电平的转换就是一个周期，称为时钟周期。

对于CPU来说，在一个时钟周期内，CPU仅能完成一个最基本的动作，时钟频率越高，时钟周期就越短，工作速度就越快。

一个时钟周期一定能执行完一条指令码？答案是不一定的，大多数指令不能在一个时钟周期完成，通常需要若干个时钟周期。不同的指令需要的时钟周期是不同的，加法和乘法都对应着一条cpu指令，但是乘法需要的时钟周期就要比加法多。

*如何让程序跑的更快？*

程序执行的时候，耗费的CPU时间少就说明程序是快的，对于程序的cpu执行时间，我们可以拆解成
**CPU时钟周期数（CPU Cycles）**和**时钟周期时间（Clock Cycle Time）的乘积**。
`程序的cpu执行时间= cpu时钟周期数 * 时钟周期时间`

时钟周期时间就是我们前面提及的cpu主频，主频越高说明cpu的工作速度就越快，比如我手头上的电脑的cpu是2.4GHz 四核 Intel Core i5，这里的2.4 GHz就是电脑的主频，时钟周期时间就是 1/2.4G。

要想CPU跑的更快，自然缩短时钟周期，也就是提升CPU主频，但是今非彼日，摩尔定律早已失效，当今的CPU主频已经很难再做到翻倍的效果了。

另外，换一个更好的cpu，这个也是我们软件工程师控制不了的使其，我们应该把目光放到另一个惩罚因子————cpu时钟周期数，如果能减少程序所需的CPU时钟周期数量，一样也是能提升程序的性能的。

对于CPU时钟周期数，我们可以进一步拆解成：`指令数*每条指令的平均时钟周期数（Cycles Per Instruction，简称 CPI）`，于是程序的CPU执行时间的公式可变成如下：
`程序的cpu执行时间= 指令数*CPI*时钟周期时间）`

因此，要想程序跑的更快，优化这三者即可：
- **指令数**：表示执行程序所需要多少条指令，以及哪些指令。这个层面是基本靠编译器来优化，毕竟同样的代码，在不同的编译器，编译出来的计算机指令会有各种不同的表示方式。
- **每条指令的平均时钟周期数CPI**：表示一条指令需要多少个时钟周期数，现代大多数CPU通过流水线技术（Pipline），让一条指令需要的CPU时钟周期数尽可能的少；
- **时钟周期时间**：表示计算机主频，取决于计算机硬件。有的CPU支持超频技术，打开了超频意味着把CPU内部的时钟给调快了，于是CPU工作速度就变快乐，但是也是有代价的，CPU跑的越快，散热的压力就会越大，CPU越容易崩溃。

#### 总结
回到最开始的问题：

##### 64位相比32位CPU的优势在哪？64位cpu的计算性能一定比32位cpu高很多吗？

64位相比32位cpu的优势主要体现在两个方面：
- 64位cpu可以一次计算超过32位的数字，而32位cpu如果要计算超过32位的数字，要分多步骤进行计算，效率就没那么高，但是大部分应用程序很少会计算那么大的数字，所以**只有运算大数字的时候，64位cpu的优势才会体现出来，否则和32位cpu的计算性能相差不大**。
- 64位cpu可以**寻址更大的内存空间**，32位cpu最大的寻址地址是4g，即使你加了8G大小的内存，也还是只能寻址到4G，而64位cpu最大寻址地址是`2^64`，远超于32位cpu最大寻址地址的`2^32`。

##### 你知道软件的32位和64位之间的区别吗？再来32位的操作系统可以运行在64位的电脑上吗？64位的操作系统可以运行在32位的电脑上吗？如果不行，原因是什么？

64位和32位软件，实际上代表指令是64位还是32位的：
- 如果32位指令在64位机器上执行，需要一套兼容机制，就可以做到兼容运行了。但是**如果64位指令在32位机器上执行，就比较困难了，因为32位的寄存器存不下64位的指令**；
- 操作系统其实也是一种程序，我们也会看到操作系统分为32位操作系统、64位操作系统，其代表意义就是擦欧总系统中程序的指令是多少位，比如64位操作系统，指令也就是64位，因此不能装在32位机器上。

总之，硬件的64位和32位指的是cpu的位宽，软件的64位和32位指的是指令的位宽。

### 1.2 存储器金字塔

大家如果想自己组装电脑的画，肯定需要买一个cpu，但是存储器方面的设备，分类比较多，那我们肯定不能只买一种存储器，比如你除了买内存，还要买硬盘，而针对硬盘我们还可以选择是固态硬盘还是机械硬盘。

相信大家都知道内存和硬盘都属于计算机的存储设备，断电后内存的数据是会丢失的，而硬盘不会，一位内硬盘是持久化存储设备，同时也是一个I/O设备。

但其实cpu内部也有存储数据的组件，这个应该比较少人注意到，比如`寄存器`、`CPU L1/L2/L3 Cache`也都是属于存储设备，只不过它们能存储的数据非常小，但是它们因为靠近cpu核心，所以访问速度都非常快，快过硬盘好几个数量级别。

问题来了，**那机械硬盘、固态硬盘、内存这三个存储器，到底和CPU L1 Cache相比速度差多少倍呢？**

在回答这个问题之前，我们先来看看**存储器的层次架构**，好让我们对存储器设备有一个整体的认识。

#### 存储器的层次结构

我们想象一个场景，大学期末准备考试了，你前去图书馆临时抱佛脚。那么，在看书的时候，我们的大脑会思考问题，也会记忆知识点，另外我们通常也会把常用的书放在自己的桌子上，当我们要找一本不常用的书，则会去图书馆的书架找。

就是这么一个小小的场景，已经把计算机的存储结构基本都涵盖了。

我们可以把cpu比喻成我们的大脑，大脑正在思考的东西，就好比cpu中的`寄存器`,处理速度是最快的，但是能存储的数据也是最少的，毕竟我们也不能一下同时思考太多的事情，除非你练过。

我们大脑中的记忆，就好比`CPU Cache`，中文称为CPU高速缓存，处理速度相比寄存器慢了一点，但是能存储的数据也稍微多了一些。

CPU Cache通常会分为`L1、L2、L3三层`，其中L1 Cache通常分为“数据缓存”和“指令缓存”，L1是距离CPU最近的，因此它比L2、L3的读写速度都快、存储空间都小。我们大脑中短期记忆，就好比L1 Cache，而长期记忆好比L2/L3 Cache。

寄存器和CPU Cache都是在CPU内部，跟CPU挨着很近，因此它们的读写速度都相当的快，但是能存储的数据很少，毕竟CPU就这么丁点大。

知道CPU内部的存储器的层次分布，我们放眼看CPU外部的存储器。

当我们大脑记忆中没有资料的时候，可以从书桌或书架上拿书来阅读，那我们桌子上的书，就好比`内存`，我们虽然可以一伸手就可以拿到，但读写速度肯定远慢于寄存器，那图书馆书架上的书，就好比`硬盘`，能存储的数据非常大，但是读写速度相比内存差好几个数量级，更别说寄存器的差距了。

我们从图书馆书架取书，把书放到桌子上，在阅读书，我们但那哦就会记忆知识点，然后再经过大脑思考，这一系列过程，相当于，数据从硬盘加载到内存，再从内存加载到CPU的寄存器和Cache中，然后再通过CPU进行处理和计算。

**对于存储器，它的速度越快、能耗会越高、而且材料的成本也是越贵的，以至于速度快的存储器的容量都比较小。**

CPU的寄存器和Cache，是整个计算机存储器中价格最贵的，虽然存储空间很小，但是读写速度是极快的，而相比较便宜的内存和硬盘，速度肯定比不上CPU内部的存储器，但是能弥补存储空间的不足。

存储器通常可以分为这么几个级别：
- 寄存器；
- CPU Cache：
  1. L1-Cache
  2. L2-Cache
  3. L3-Cache
- 内存；
- SSD/HDD 硬盘

##### 寄存器

最靠近CPU的控制单元和逻辑计算单元的存储器，就是寄存器了，它使用的材料速度也是最快的，因此价格也是最贵的，那么数量不能很多。

存储器的数量通常在几十到几百之间，每个寄存器可以用来存储一定的字节（byte）的数据。比如：
- 32位cpu中大多数寄存器可以存储*4*个字节
- 64位cpu中大多数寄存器可以存储*8*个字节

寄存器的访问速度非常快，一般要求在半个CPU时钟周期内完成读写，CPU时钟周期跟CPU主频息息相关，比如2GHz主频的CPU，那么它的时钟周期就是1/2G，也就是0.5ns。

CPU处理一条指令的时候，除了读写寄存器，还需要解码指令、控制指令执行和计算。如果寄存器的速度太慢，则会拉长指令的处理周期，从而给用户的感觉，就是电脑“很慢”。

##### CPU Cache
CPU Cache用的是一种叫**SRAM（Static Random-Access Memory，静态随机存储器）**的芯片。

SRAM之所以叫“静态”存储器，是因为只要有电，数据就可以保持存在，而一旦断电，数据就会丢失了。

在SRAM里面，一个bit的数据，通常需要6个晶体管，所以SRAM的存储密度不高，同样的物理空间下，能存储的数据是有限的，不过也因为SRAM的电路简单，所以访问速度非常快。

CPU的高速缓存，通常可以分为L1、L2、L3这样的三层高速缓存，也成为一级缓存、二次缓存、三次缓存。


###### L1 高速缓存

L1 高速缓存的访问速度几乎和寄存器一样快，通常只需要2~4个时钟周期，而大小在几十KB到几百KB不等。

每个CPU核心都有一块属于自己的L1高速缓存，指令和数据在L1是分开存放的，所以L1高速缓存通常分成*指令缓存*和*数据缓存*。

在Linux系统，我们可以通过这条命令，查看CPU里的L1 Cache “数据”缓存的容量大小：
```shell
cat /sys/devices/system/cpu/cpu0/cache/index0/size
```

而查看L1 Cache “指令”缓存的容量大小，则是：
``` shell
cat /sys/devices/system/cpu/cpu0/cache/index1/size
```

###### L2 高速缓存

L2高速缓存同样每个CPU核心都有，但是L2高速缓存位置比L1高速缓存距离CPU核心更远，它大小比L1高速缓存更大，CPU型号不同大小也就不同，通常大小在几百KB到几MB不等，访问速度则更慢，速度在``10~20``个时钟周期。

在Linux系统，我们可以通过这条命令，查看CPU里的L2 Cache的容量大小：
``` shell
cat /sys/devices/system/cpu/cpu0/cache/index2/size
```

###### L3 高速缓存

L3高速缓存通常是多个CPU核心共用的，位置比L2高速缓存距离CPU核心更远，大小也会更大些，通常大小在几MB到几十MB不等，具体值根据CPU型号而定。

访问速度相对也比较慢一些，访问速度在`20~60`个时钟周期。

在Linux系统，我们可以通过这条命令，查看CPU里的L3 Cache的容量大小：
``` shell
cat sys/devices/system/cpu/cpu0/cache/index3/size
```

##### 内存

内存用的芯片和CPU Cache有所不同，它使用的是一种叫作**DRAM （Dynamic Random Access Memory，动态随机存取存储器）**的芯片。

相比SRAM，DRAM的密度更高，功耗更低，有更大的容量，而且造价比SRAM芯片便宜很多。

DRAM存储一个bit数据，只需要一个晶体管和一个电容就能存储，但是因为数据会被存储在电容里，电容会不断漏电，所以需要“定时刷新”电容，才能保证数据不会被丢失，这就是DRAM之所以被称为“动态”存储器的原因，只有不断刷新，数据才能被存储起来。

DRAM的数据访问电路和刷新电路都比SRAM更复杂，所以访问的速度会更慢，内存速度大概在`200~300`个时钟周期之间。

##### SSD/HDD硬盘

SSD（Solid-state disk）就是我们常说的固体硬盘，结构和内存类似，但是它相比内存的优点是断电后数据还是存在的，而内存、寄存器、高速缓存断电后数据都会丢失。内存的读写速度比SSD大概快`10~1000`倍。

当然，还有一款传统的硬盘，也就是机械硬盘（Hard Disk Drive，HDD），它是通过物理读写的方式来访问数据的，因此它访问速度是非常慢的，它的速度比内存慢*10W*倍左右。

由于SSD的加个快接近机械硬盘了，因此机械硬盘已经逐渐被SSD替代了。

#### 存储器的层次关系

现代的一台计算机，都用上了CPU Cache、内存、到SSD或HDD硬盘这些存储器设备了。

其中，存储空间越大的存储器设备，其访问速度越慢，所需成本也相对越少。

CPU并不会直接和每一种存储器设备直接打交道，而是每一种存储器设备只和它相邻的存储器设备打交道。

比如，CPU Cache的数据是从内存加载过来的，写回数据的时候也只写回到内存，CPU Cache不会直接把数据写到硬盘，也不会直接从硬盘加载数据，而是先加载到内存，再从内存加载到CPU Cache中。

所以，**每个存储器只和相邻的一层存储器设备打交道，并且存储设备为了追求更快的速度，所需的材料成本必然也是跟高，也正因为成本太高，所以CPU内部的寄存器、L1\L2\L3 Cache 只好用较小的容量，相反内存、硬盘则可用更大的容量，这就是我们今天所说的存储器层次结构。**

另外，当CPU需要访问内存中某个数据的时候，如果寄存器有这个数据，CPU就直接从寄存器取数据即可，如果寄存器没有这个数据，CPU就会查询L1高速缓存，如果L1没有，则查询L2高速缓存，L2还是没有的话就查询L3高速缓存，L3依然没有的话，才去内存中取数据。

所以，存储层次结构也形成了缓存的体系。

#### 存储器之间的实际价格和性能差距

前面我们知道了，速度越快的存储器，造价成本往往也越高，那我们就以实际的数据来看看，不同层级的存储器之间的性能和价格差异。

|存储器|硬件介质| 单位成本<br>(美元/MB)|随机访问延时|
|------|----|----|----|
|L1 Cache|SRAM|7|1 ns|
|L2 Cache|SRAM|7|4 ns|
|Memory|DRAM|0.015|100ns|
|Disk|SSD(NAND)|0.0004|150 us|
|Disk|HDD|0.00004|10ms|

可以看到L1 Cache的访问延时是1纳秒，而内存以及是100纳秒了，相比L1 Cache速度慢了*100*倍。另外，机械硬盘的访问延时更是高达10毫秒，相比L1 Cache 速度慢了*10000000*倍，差了好几个数量级别。

在价格上，每生成MB大小的L1 Cache相比内存贵了*466*倍，相比机械硬盘那更是贵了*175000*倍。

#### 总结

各种存储器之间的关系，可以用图书馆学习这个场景来理解。

CPU可以比喻成我们的大脑，我们当前正在思考和处理的只是的过程，就好比CPU中的*寄存器*处理数据的过程，速度极快，但是容量很小。而cpu中的*L1-L3 Cache*就好比我们大脑中的短期记忆和长期记忆，需要小小花费点时间来调取数据并处理。

我们面前的桌子就相当于*内存*，能放下更多的书（数据），但是找起来和看起来就需要花费一些时间，相比CPU Cache 慢不少。而图书馆的书架相当于*硬盘*，能放下比内存更多的数据，但找起来就更花费时间了，可以说是最慢的存储器设备了。

从寄存器、CPU Cache，到内存、硬盘，这样一层层下来的存储器，访问速度越来越慢，存储容量越来越大，价格也越来越便宜，而且每个存储器只和相邻的一层存储设备打交道，于是这样就形成了存储器的层次结构。

再来回答，开头的问题：`那机械硬盘、固态硬盘、内存这三个存储器，到底和*CPU L1 Cache*相比速度差多少倍呢？`

CPU L1 Cache随机访问延时是1纳秒，内存则是100纳秒，所以**CPU L1 Cache 比内存 快 ***100***倍左右**。

SSD随机访问延时是150微秒，所以**CPU L1 Cache 比SSD快15000倍左右**。

最慢的机械硬盘随机访问延时已经高达10毫秒，我们来看看机械硬盘到底有多“龟速”：
- SSD比机械硬盘快70倍左右；
- 内存比机械硬盘快100000倍左右；
- CPU L1 Cache比机械硬盘快10，000，000倍左右

我们把上述的时间比例差异放大后，就能非常直观感受到它们的性能差异了。如果CPU访问L1 Cache的缓存时间是1秒，那访问内存则需要大约2分钟，随机访问SSD里的数据则需要1.7天，访问机械硬盘那更久，长达近4个月。

可以发现，不同的存储器之间性能差距很大，构造存储器分级很有意思，分级的目的是要构造**缓存**体系。

### 1.3 如何写出让CPU跑得更快的代码？

代码都是由CPU跑起来的，我们代码写得好与坏就决定了CPU的执行效率，特别是在编写计算密集型的程序，更要注重CPU的执行效率，否则将会大大影响系统性能。

CPU内部嵌入了CPU Cache，它的存储容量很小，但是离CPU核心很近，所以缓存的读写速度是极快的，那么如果CPU运算时，直接从CPU Cache读取数据，而不是从内存的话，运算速度就会很快。

但是，大多数人不知道CPU Cache的运行机制，以至于不知道如何才能够写出配合CPU Cache工作机制的代码，一旦掌握了它，写代码的时候，就有新的优化思路了。

那么接下来我们就来看看，CPU Cahce到底是什么样的，是如何工作的，又该如何写出让CPU执行更快的代码。

#### CPU Cahce 有多快？

你可能会好奇为什么有了内存，还需要CPU Cache？根据摩尔定律，CPU的访问速度每18个月就会翻倍，相当于每年增长60%左右，内存的速度当然也会不断增长，但是增长的速度远小于CPU，平均每年只增长7%左右。于是，CPU与内存的访问性能的差距不断拉大。

到现在，一次内存访问所需时间是**200-300**多个时钟周期，这意味着CPU和内存的访问速度已经相差**200-300**多倍了。

为了弥补CPU与内存两者之间的性能差距，就在CPU内部引入了CPU Cache，也称高速缓存。

CPU Cache通常分为大小不等的三级缓存，分别是**L1 Cache、L2 Cache和L3 Cache**。

由于CPU Cache所使用的材料是SRAM，加个比内存使用的DRAM高出很多，在当今每生产1MB大小的CPU Cache需要7美金的成本，而内存只需要0.015美金的成本，成本方面相差了466倍，所以CPU Cache不像内存那样动辄以GB计算，他的大小以KB或MB来计算的。

在Linux系统中，我们可以使用下图的方式来查看各级CPU Cache的大小，比如这台服务器，离CPU核心最近的L1 Cache是32KB，其次L2 Cache是256KB，最大的L3 Cache则是3MB。

其中，**L1 Cache 通常会分为“数据缓存”和“指令缓存”**，这意味着数据和指令在L1 Cache这一层是分开缓存的，`index0`也就是数据缓存，而`index1`则是指令缓存，它俩的大小通常是一样的。

另外，你也会注意到，L3 Cache比L1 Cache和L2 Cache大很多，这是因为**L1 Cache 和L2 Cache 都是每个CPU核心独有的，而L3 Cache是多个CPU核心共享的**。

程序执行时，会先将内存中的数据加载到共享的L3 Cache中，再加载到每个核心独有的L2 Cache，最后进入到最快的L1 Cache，之后才会被CPU读取。它们之间的层级关系如下图：
```mermaid
flowchart TB
    subgraph CPU
        subgraph "核心 1"
            L1D1["L1 数据缓存"]
            L1I1["L1 指令缓存"]
            L21["L2 缓存"]
        end
        subgraph "核心 2"
            L1D2["L1 数据缓存"]
            L1I2["L1 指令缓存"]
            L22["L2 缓存"]
        end
        L3["L3 缓存"]
    end

    L1D1 <--> L21
    L1I1 <--> L21
    L1D2 <--> L22
    L1I2 <--> L22
    L21 <--> L3
    L22 <--> L3
```

越靠近CPU核心的缓存其访问速度越快，CPU访问L1 Cache只需要*2-4*个时钟周期，访问L2 Cache大约*10-20*个时钟周期，访问L3 Cache大约*20-60*个时钟周期，而访问内存速度大概再*200-300*个时钟周期之间。

**所以，CPU从L1 Cache 读取数据的速度，相比从内存读取的速度，会快 100 多倍。**

#### CPU Cache的数据结构和读取过程是什么样的？

CPU Cache的数据是从内存读取过来的，它是以一小块一小块读取数据的，而不是按照单个数组元素来读取数据的，在CPU Cache中的这样一小块一小块的数据，称为`Cache Line（缓存块）`。

可以在Linux系统，用下面这种方式来查看CPU的Cache Line的**一次载入数据的大小（字节byte）**
```shell
cat /sys/devices/system/cpu/cpu0/cache/idnex0/coherency_line_size
```

比如，有一个 `` int array[100] ``的数组，当载入`array[0]`时，由于这个数组元素的大小在内存只占4字节，不足64字节，CPU就会**顺序加载**数组元素到`array[15]`，意味着`array[0]-array[15]`数组元素都会被缓存在CPU Cache中了，因此当下次访问这些数组元素时，会直接从CPU Cache读取，而不用再从内存中读取，大大提高了CPU读取数据的性能。

事实上，CPU读取数据的时候，无论数据是否存放到Cache中，CPU都是先访问Cache，只有当Cache中找不到数据时，才会去访问内存，并把内存中的数据读入Cache中，CPU再从CPU Cache读取数据。

这样的访问机制，跟我们使用“内存作为硬盘的缓存”的逻辑是一样的，如果内存有缓存的数据，则直接返回，否则要访问龟速一般的硬盘。

那CPU怎么知道要访问的内存数据，是否在Cache里？如果在的话，如何找到Cache对应的数据呢？我们从最简单、基础的**直接映射Cache（direct Mapped Cache）** 说起，来看看整个CPU Cache的数据结构和访问逻辑。

前面，我们提到CPU访问内存数据时，时一小块一小块数据读取的，具体这一小块数据的大小，取决于`coherency_line_size`的值，一般64字节。在内存中，这一块的数据我们称为**内存块（Block）**，读取的时候我们要拿到数据所在内存块的地址。

对于直接映射Cache采取的策略，就是把内存块的地址始终“映射”在一个CPU Line（缓存块）的地址，至于映射关系实现方式，则是使用“取模运算”，把取模运算的结果就是内存块地址对应的CPU Line（缓存块）的地址。

举个例子，内存共被划分为32个内存块，CPU Cache共有8个CPU Line，假设CPU想要访问第15号内存块，如果15号内存块中的数据已经缓存在CPU Line中的话，则是一定映射在7号CPU Line中，因为`15%8`的值是7。

机制的你肯定发现了，使用取模方式映射的话，就会出现多个内存块对应同一个CPU Line，比如上面的例子，除了15号内存块是映射在7号CPU Line中，还有7号、23号、31号内存块都是映射到7号CPU Line中。

因此，为了区别不同的内存块，在对应的CPU Line中我们还会存储一个**组标记（Tag）**。这个组标记会记录当前CPU Line中存储的数据对应的内存块，我们可以用这个组标记来区分不同的内存块。

除了组标记信息外，CPU Line还有两个信息：

- 一个是，从内存加载过来的实际存放**数据（Data）**。
- 另一个是，**有效位（Valid bit）**，它是用来标记对应的CPU Line中的数据是否是有效的，如果有效位是0，无论CPU Line中是否有数据，CPU都会直接访问内存，重新加载数据。

CPU在从CPU Cache读取数据的时候，并不是读取CPU Line中的整个数据块，而是读取CPU所需要的一个数据片段，这样的数据统称为一个**字（Word）**。那怎么在对应的CPU Line中数据块中找到所需的字呢？答案是，需要一个**偏移量（Offset）**。

因此，一个内存的访问地址，包括**组标记、CPU Line 索引、偏移量**这三种信息，于是CPU就能通过这些信息，在CPU Cache中找到缓存的数据。而对于CPU Cache里的数据结构，则是由**索引+有效位+组标记+数据块**组成。

如果内存中的数据已经在CPU Cache中了，那CPU访问一个内存地址的时候，会经历这4个步骤：
1. 根据内存地址中索引信息，计算在CPU Cache中的索引，也就是找出对应的CPU Line的地址；
2. 找到对应的CPU Line后，判断CPU Line中的有效位，确认CPU Line中数据是否是有效的，如果是无效的，CPU就会直接访问内存，并重新加载数据，如果数据有效，则往下执行；
3. 对比内存地址中组标记和CPU Line中的组标记，确认CPU Line中的数据就是我们要访问的内存数据，如果不是的话，CPU就会直接访问内存，并重新加载数据，如果是的话，则往下执行；
4. 根据内存地址中偏移量信息，从CPU Line的数据块中，读取对应的字。

到这里，相信你对直接映射Cache有了一定认识，但其实除了直接映射Cache之外，还有其他通过内存地址找到CPU Cache中的数据的策略，比如全相连Cache（Fully Associative Cache）、组相连Cache（Set Associative Cache）等，这几种策略的数据结构都比较相似，我们理解了直接映射Cache的工作方式，其他的策略如果你有兴趣去看，相信很快就能理解的了。

#### 如何写出让CPU跑得更快的代码？

我们知道CPU访问内存的速度，比访问CPU Cache的速度慢了100多倍，所以如果CPU所要操作的数据在CPU Cache中的话，这样将会带来很大的性能提升。访问的数据在CPU Cache中的话，意味着**缓存命中**，缓存命中率越高的话，代码的性能就会越好，CPU也就跑的越快。

于是，“如何写出让CPU跑得更快地的代码？”这个问题，可以改成“如何写出CPU缓存命中率高的代码？”。

在前面也提到，L1 Cache通常分为数据缓存和指令缓存，这是一位内CPU会处理数据和指令，比如`1+1=2`这个运算，`+`就是指令，会被放在指令缓存中，而输入数字`1`则会被放在数据缓存里。

因此，**我们要分开看数据缓存和指令缓存的缓存命中率**。

##### 如何提升数据缓存的命中率？

假设要遍历二维数组，有以下两种形式，虽然代码执行结果一样，但你觉得哪种形式效率最高呢？为什么高呢？

```c
array[N][N] = 0;
```
```c
// 形式一：
for(i=0;i<N;i+=1){
  for(j=0;j<N;j+=1>){
    array[i][j]=0;
  }
}
```
``` c
// 形式二：
for(i=0;i<N;i+=1){
  for(j=0;j<N;j+=1>){
    array[j][i]=0;
  }
}
```

经过测试，形式以```array[i][j]```执行时间比```array[j][i]```块好几倍。

之所以有这么大的差距，是因为二维数组`array`所占用的内存是连续的，比如长度`N`的值是`2`的话，那么内存中的数组元素的布局顺序是这样的:
```c
// 内存中的数组元素的布局顺序
array[0][0],array[0][1],array[1][0],array[1][1]
```

形式一用 array[i][j]访问数组元素的顺序，正是和内存中数组元素存放的顺序一致。当CPU访问 array[0][0]时，由于数据不在Cache中，于是会顺序把跟随其后的3个元素从内存中加载到CPU Cache，这样当CPU访问后面的3个数组元素时，就能在CPU Cache中成功找到数据，这意味着缓存命中率很高，缓存命中的数据不需要访问内存，这便大大提高了代码性能。

而如果用形式二的 `array[j][i]`来访问，可以看到访问的方式是跳跃式的，而不是顺序的，那么如果N的数值很大，那么操作array[j][i]时，是没办法把array[j+1][i]也读入CPU Cache中的，既然array[j+1][i]没有读取到CPU Cache，那么就需要从内存读取该数据元素了。很明显，这种不连续性、跳跃式访问数据元素的方式，可能不能充分利用到了CPU Cache的特性，从而代码的性能不高。

那访问 array[0][0]元素时，CPU具体会一次从内存加载多少元素到CPU Cache呢？这个问题，在前面我们也提到过，这跟CPU Cache Line有关，他表示CPU Cache一次性 能加载数据的大小，可以在Linux里通过`coherency_line_size`配置查看它的大小，通常是64个字节。

也就是说，当CPU访问内存数据时，如果数据不在CPU Cache中，则会一次性连续加载64字节大小的数据到CPU Cache，那么当访问array[0][0]时，由于该元素不足64字节，于是就会往后*顺序*读取array[0][0]-array[0][15]到cpu cache中。顺序访问的array[i][j]因为利用了这一特点，所以就会比跳跃式访问的array[j][i]要快。

**因此，遇到这种遍历数组的情况时，按照内存布局顺序访问，将可以有效的利用CPU Cache带来的好处，这样我们代码的性能就会得到很大的提升**

##### 如何提升指令缓存的命中率？

提升数据的缓存命中率的方式，是按照内存布局顺序访问，那针对指令的缓存该如何提升呢？

我们以一个例子来看看，有一个元素位0到100之间随机数字组成的一维数组：
``` c
int array[N];
for(i=0;i<N;i++){
  array[i]=rand()%100;
}
```

接下来，对这数组做两个操作：
```c
for(i=0;i<N;i++){
  if(array[i]<50){
    array[i]=0;
  }
}

sort(array,array+N);
```
- 第一个操作：循环遍历数组，把小于50的数组元素置为0；
- 第二个操作：将数组排序；

那么问题来了，你觉得先遍历在排序速度快，还是先排序再遍历速度快呢？

再回答这个问题之前，我们先了解CPU的**分支预测器**。对于if条件语句，意味着此时至少可以选择跳转到两段不同的指令执行，也就是if还是else中的指令。那么，**如果分支预测可以预测接下来要执行if里的指令，还是else指令的话，就可以提前把这些指令放在指令缓存中，这样CPU就可以直接从Cache读取到指令，于是执行速度就会很快。**

当数组中的元素时随机的，分支预测就无法有效工作，而数组元素都是顺序的，分支预测器会动态地根据历史命中数据对未来进行预测，这样命中率就会很高。

因此，先排序再遍历速度会更快，这是因为排序之后，数字是从小到大的，那么前几次循环命中 `if<50`的次数会比较多，于是分支预测就会缓存`if`里的`array[i]=0`指令到Cache中，后续CPU执行该指令就只需要从Cache读取就好了。

如果你肯定代码中的if表达式判断为true的概率比较高，我们可以使用显示分支预测工具，比如在C/C++语言中编译器提供了likely和unlikely这两种宏，如果if条件为true的概率大，则可以把likely宏if里的表达式包裹起来，反之用unlikely宏。

实际上，CPU自身的动态分支预测已经是比较准的了，所以只有当非常确信CPU预测的不准，且能够知道实际的概率情况时，才建议使用这两种宏。

##### 如何提升多核CPU的缓存命中率

在单核CPU，虽然只能执行一个进程，但是操作系统给每个进程分配了一个时间片，时间片用完了，就调度下一个进程，于是各个进程就按时间片交替地占用CPU，从宏观上看起来各个进程同时在执行。

而现代CPU都是多核心的，进程可能在不同CPU核心来回切换执行，这对CPU Cache不是有利的，虽然L3 Cache是多核心之间共享的，但是L1和L2 Cache都是每个核心独有的，**如果一个进程在不同核心来回切换，各个核心的缓存命中率就会受到影响**，相反如果进程都在同一个核心上执行，那么其数据的L1和L2 Cache的缓存命中率可以得到有效提高，缓存命中率就意味着CPU可以减少访问内存的频率。

当有多个同时执行“计算密集型”的线程，为了防止因为切换到不同的核心，从而导致缓存命中率下降的问题，可以把**线程绑定在某一个CPU核心上**，这样性能可以得到非常可观的提升。

在Linux上提供了`sched_setaffinity`方法，来实现将线程绑定到某个CPU核心这一功能。

#### 总结

由于随着计算机技术的发展，CPU与内存的访问速度相差越来越多，如今差距已经高达好几百倍了，所以CPU内部嵌入了CPU Cache组件，作为内存与CPU之间的缓存层，CPU Cache由于离CPU核心很近，所以访问速度也是非常快的，但由于所需材料成本比较高，它不像内存动辄几个GB大小，而是仅有几十KB到MB大小。

当CPU访问数据的时候，先是访问CPU Cache，如果缓存命中的话，则直接返回数据，就不用每次都从内存读取了。因此，缓存命中率越高，代码的性能越好。

但需要注意的是，当CPU访问数据时，如果CPU Cache没有缓存该数据，则会从内存读取数据，但是并不是只读取一个数据，而是一次性读取一块一块的数据存放到CPU Cache中，之后才会被CPU读取。

内存地址映射到CPU Cache地址里的策略有很多种，其中比较简单是直接映射Cache，它巧妙的把内存地址拆分成“索引+组标记+偏移量”的方式，使我们可以将很大的内存地址，映射到很小的CPU Cache地址里。

要想写出让CPU跑得更快的代码，就需要写出缓存命中率高的代码，CPU L1 Cache 分为数据缓存和指令缓存，因而需要分别提高它们的缓存命中率：
- 对于数据缓存，我们在遍历数据的时候，应该按照内存布局的顺序操作，这是因为CPU Cache是根据CPU Cache Line批量操作数据的，所以顺序地操作连续内存数据时，性能能得到有效的提升；
- 对于指令缓存，有规律的条件分支语句能够让CPU的分支预测器发挥作用，进一步提高执行的效率；

另外，对于多核CPU系统，线程可能在不同CPU核心来回切换，这样各个核心的缓存命中率就会受到影响，于是要想提高进程的缓存命中率，可以考虑把线程绑定CPU到某一个CPU核心。

### 1.4 CPU缓存一致性

#### CPU Cache的数据写入

随着时间的推移，CPU和内存的访问性能相差越来越大，于是就在CPU内部嵌入了CPU Cache，CPU Cache离CPU核心相当近，因此它的访问速度是很快的，于是它充当了CPU与内存之间的缓存角色。

CPU Cache通常分为三级缓存：L1 Cache、L2 Cache、L3 Cache，级别越低的离CPU核心越近，访问速度也越快，但是存储容量相对就会越小。其中，在多核心的CPU里，每个核心都有各自的L1/L2 Cache，而L3 Cache是所有核心共享使用的。

我们先简单了解下CPU Cache的构造，CPU Cache是由很多个Cache Line组成的，CPU Line是CPU 从内存读取数据的基本单位，而 CPU Line是各种标志（Tag）+数据块（Data Block）组成。

我们当然期望cpu读取数据的时候，都是尽可能地从CPU Cache中读取，而不是每次都要从内存中获取数据。所以，身为程序员，我们要尽可能写出缓存命中率高的代码，这样就有效提高程序的性能，具体的做法，可以参考上一章“如何写出让CPU跑得更快的代码？”

事实上，数据不光是只有读操作，还有写操作，那么如果数据写入Cache之后，内存与Cache相对应的数据将会不同，这种情况下Cache和内存数据都不一致了，于是我们肯定要把Cache的数据同步到内存里的。

问题来了，那在什么时机才把Cache中的数据写回到内存呢？为了对应这个问题，下面介绍两种针对写入数据的方法：
- 写直达（Write Through）
- 写回（Write Back）

##### 写直达

保持内存与Cache一致性最简单的方式是，**把数据同时写入内存和Cache中**，这种方法称为**写直达**。

在这个方法里，写入前会先判断数据是否已经在CPU Cache里面了：
- 如果数据已经在Cache里面，先将数据更新到Cache里面，再写入到内存里面；
- 如果数据没有在Cache里面，就直接把数据更新到内存里面。

写直达法很直观，也很简单，但是问题明显，无论数据在不在Cache里面，每次写操作都会写回到内存，这样写操作会花费大量时间，无疑性能会收到很大的影响。

##### 写回

既然写直达由于每次写操作都会把数据写回到内存，而导致影响性能，于是为了要减少数据写回内存的频率，就出现了**写回**的方法。

在写回机制中，**当发生写操作时，新的数据仅仅被写入Cache Block里，只有当修改过的Cache Block“被替换”时才需要写到内存中**，减少了数据写回内存的频率，这样便可以提高系统的性能。

那具体如何做到的呢？下面来详细说一下：
- 如果当发生写操作时，数据已经在CPU Cache里的话，则把数据更新到CPU Cache里，同时标记CPU Cache里的这个Cache Block为脏（Dirty）的，这个脏的标记代表这个时候，我们CPU Cache里面的这个Cache Block的书和内存是不一致的，这种情况是不用把数据写到内存里的；
- 如果发生写操作时，数据所对应的 Cache Block 里存放的是“别的内存地址的数据”的话，就要检查这个Cache Block 里的数据有没有有被标记为脏的，如果是脏的话，我们就要这个 Cache Block 里的数据写回到内存，然后再把当前要写入的数据，写入到这个Cache Block 里，同时也把它标记为脏的；如果Cache Block里面的数据没有被标记为脏的，则就直接将数据写入到这个Cache Block里，然后再把这个Cache Block标记为脏的就好了。

可以发现写回这个方法，在把数据写入到Cache的时候，只有在缓存不命中，同时数据对应的Cache中的Cache Block为脏标记的情况下，才会把数据写到内存中，而在缓存命中的情况下，则在写入Cache后，只需要把该数据对应的Cache Block标记为脏即可，而不用写到内存里。

这样的好处是，如果我们大量的操作都能够命中缓存，那么大部分时间里CPU都不需要读写内存，自然性能相比写直达会高很多。

#### 缓存一致性问题

现在CPU都是多核的，由于L1/L2 Cache是多个核心各自独有的，那么会带来多核心的**缓存一致性（Cache Coherennce）**的问题，如果不能保证缓存一致性的问题，就可能造成结果错误。

那缓存一致性的问题具体是怎么发生的呢？我们以一个含有两个核心的CPU作为例子看一看。

假设A号核心和B号核心同时运行两个线程，都操作共同的变量i（初始值为0）。

这时如果A号核心执行了`i++`语句的时候，为了考虑性能，使用了我们前面所说的写回策略，先把值为`1`的执行结果写入到L1/L2 Cache中，然后把L1/L2 Cache中对应的Block标记为脏的，这个时候数据其实没被同步到内存中的，因为写回策略，只有在A号核心中的这个Cache Block要被替换的时候，数据才会写入到内存里。

如果这时旁边的B号核心尝试从内存读取i变量的值，则读到的将会是错误的值，因为刚才A号核心更新i值还没写入到内存中，内存中的值还依然是0。**这个就是所谓的缓存一致性问题，A号核心和B号核心的缓存，在这个时候是不一致，从而会导致执行结果的错误。**

那么，要解决这一问题，就需要一种机制，来同步两个不同核心里面的缓存数据。要实现的这个机制的话，要保证做到下面这2点：
1. 某个cpu核心里的Cache数据更新时，必须要传播其他核心的Cache，这个称为**写传播（Write Propagation）**；
2. 某个CPU核心里对数据的操作顺序，必须在其他核心看起来顺序是一样的，这个称为**事务的串行化（Transaction Serialization）**。

第一点写传播很容易就理解，当某个核心在Cache更新了数据，就需要同步到其他核心的Cache里。而对于第二点事务的串行化，我们举个例子来理解它。

假设我们有一个含有4个核心的CPU，这4个核心都操作共同的变量i（初始值为0）。A号核心先把i值变为100，而此时同一时间，B号核心先把i值变为200，这里两个修改，都会“传播”到c和d号核心。

那么问题来了，C号核心先收到了A号核心更新数据的事件，在收到B号核心更新数据的事件，因此C号核心看到的变量i是先变成100，后变成200.

而如果D号核心收到的事件是反过来的，则D号核心看到的是变量i先变成200，再变成100，虽然是做到了写传播，但是各个Cache里面的数据还是不一致的。

所以，我们要保证C号核心呵呵D号核心都能看到**相同顺序的数据变化**，比如变量i都是先变成100，再变成200，这样的过程就是事务的串行化。

要实现事务串行化，要做到2点：
- CPU核心对于Cache中数据的操作，需要同步给其他CPU核心；
- 要引入“锁”的概念，如果两个CPU核心里由相同数据的Cache，那么对于这个Cache数据的更新，只有拿到了“锁”，才进行对应的数据更新。

那接下来我们看看，写传播和事务串行化具体是用什么技术实现的。

#### 总线嗅探

写传播的原则就是当某个CPU核心更新了Cache中的数据，要把事件广播通知到其他核心。最常见实现的方式是**总线嗅探（Bus Snooping）**。

还是以前面的i变量例子来说明总线嗅探的工作机制，当A号CPU核心修改了L1 Cache中i变量的值，通过总线把这个事件广播通知给其他所有的核心，然后每个CPU核心都会监听总线上的广播事件，并检查是否由相同的数据在自己的L1 Cache里面，如果B号CPU核心的L1 Cache由该数据，那么也需要把该数据更新到自己的L1 Cache。

可以发现，总线嗅探方法很简单，CPU需要每时每刻监听总线上的一切活动，但是不管别的核心的Cache是否缓存相同的数据，都需要发出一个广播事件，这无疑会加重总线的负载。

另外，总线嗅探只是保证了某个CPU核心的Cache更新数据这个事件能被其他CPU核心知道，但是并不能保证事务串行化。

于是，有一个协议基于总线嗅探机制实现了事务串行化，也用状态机机制降低了总线带宽压力，这个协议就是MESI协议，这个协议就做到了CPU缓存一致性。

#### MESI协议

MESI协议其实是4个状态单词的开头字母缩写，分别是：
- Modified，已修改
- Exclusive，独占
- Shared，共享
- Invalidated，已失效

这四个状态来标记Cache Line 四个不同的状态。

*已修改*状态就是我们前面提到的脏标记，代表该Cache Block上的数据已经被更新过，但是还没有写到内存里。而*已失效*状态，表示的是这个Cache Block里的数据已经失效了，不可以读取该状态的数据。

*独占*和*共享*状态都代表Cache Block里的数据是干净的，也就是说，这个时候Cache Block里的数据和内存里面的数据是一致性的。

*独占*和*共享*的差别在于，独占状态的时候，数据只存储在一个CPU核心的Cache里，而其他CPU核心的Cache没有该数据。这个时候，如果要向独占的Cache写数据，就可以直接自由地写入，而不需要通知其他CPU核心，一位内只有你这有这个数据，就不存在缓存一致性的问题了，于是就可以随便操作该数据。

另外，在*独占*状态下的数据，如果有其他核心从内存读取了相同的数据到各自的Cache，那么这个时候，独占状态下的数据就会变成共享状态。

那么，*共享*状态代表着相同的数据在多个CPU核心的Cache里都有，所以当我们要更新Cache里面的数据的时候，不能直接修改，而是要先向所有的其他CPU核心广播一个请求，要求先把其他核心的Cache中对应的Cache Line标记为*无效*状态，然后再更新当前Cache里面的数据。

举个具体的例子来看看这四个状态的转换：
1. 当A号CPU核心从内存读取变量i的值，数据被缓存在A号CPu核心自己的Cache里面，此时其他CPU核心的Cache没有缓存该数据，于是标记Cache Line状态为*独占*，此时其Cache中的数据和内存是一致的。
2. 然后B号CPU核心也从内存读取了变量i的值，此时会发送消息给其他CPU核心，由于A号CPU核心已经缓存了该数据，所以会把数据返回给B号CPU核心。在这个时候，A和B核心缓存了相同的数据，Cache Line的状态就会变成*共享*，并且其Cache中的数据与内存也是一致的；
3. 当A号CPU核心要修改Cache中i变量的值，发现数据对应的Cache Line的状态是共享状态，则要向所有的其他CPU核心广播一个请求，要求先把其他核心的Cache中对应的Cache Line标记为*无效*状态，然后A号CPU核心才更新Cache里面的数据，同时标记Cache Line为*已修改*状态，此时Cache中的数据就与内存不一致了。
4. 如果A号CPU核心*继续*修改Cache中i变量的值，由于此时的Cache Line是*已修改*状态，因此不需要给其他CPU核心发送消息，直接更新数据即可。
5. 如果A号CPU核心的Cache里的i变量对应的Cache Line要被*替换*，发现Cache Line状态是*已修改*状态，就会在替换前先把数据同步到内存。

所以，可以发现当Cache Line状态是*已修改*或者*独占*状态时，修改更新其数据不需要发送广播给其他CPU核心，这在一定程度上减少了总线带宽压力。

事实上，整个MESI的状态可以用一个有限状态机来表示它的状态流转。还有一点，对于不同状态触发的事件操作，可能是来自本地CPU核心发出的广播事件，也可以是来自其他CPU核心通过总线发出的广播事件。

MESI协议的四种状态之间的流转过程，汇总成下面的表格

#### 总结

CPU在读写数据的时候，都是在CPU Cache读写数据的，原因是Cache 离CPU很久，读写性能相比内存高出很高。对于Cache里么有缓存CPU所需要读取的数据的这种情况，CPU则会从内存读取数据，并将数据缓存到Cache里面，最后CPU再从Cache读取数据。

而对于数据的写入，CPU都会先写入到Cache里面，然后再在找个合适的时机写入到内存，那就有*写直达*和*写回*两种策略保证Cache与内存的数据一致性：
- 写直达，只要有数据写入，都会直接把数据写入到内存里面，这种方式简单直观，但是性能就会受限于内存的访问速度；
- 写回，对于已经缓存在 Cache 的数据的写入，只需要更新其数据就可以，不用写入到内存，只有在需要把缓存里面的脏数据交换出去的时候，才把数据同步到内存里，这种方式在缓存命中率高的情况，性能会更好；

当今CPU都是多核的，每个核心都有各自独立的L1/L2 Cache，只有L3 Cache是多个核心之间共享的。所以，我们要确保多核缓存时一致性的，否则会出现错误的结果。

要想实现缓存一致性，关键是要满足2点：
- 第一点是写传播，也就是当某个CPU核心发生写入操作时，需要把事件广播通知给其他核心；
- 第二点是事物的串行化，这个很重要，只有保证了这个，才能保障我们的数据是真正一致的，我们的程序在各个不同的核心上运行的结果也是一致的；

基于总线嗅探机制的MESI协议，就满足上面这两点，因此它是保障缓存一致性的协议。

MESI，是已修改、独占、共享、已失效这四个状态的英文缩写的组合。整个MSI状态的变更，则是根据来自本地CPU核心的请求，或者来自其他CPU核心通过总线传输过来的请求，从而构成一个流动的状态机。另外对于在已修改或者独占状态的Cache Line，修改更新其数据不需要发送广播给其他CPU核心。

### 1.5 CPU是如何执行任务的

本章要讨论的问题：
- 有了内存，为什么还需要CPU Cache？
- CPu是怎么读写数据的？
- 如何让CPU读取数据更快一些？
- CPU伪共享是如何发生的？又该如何避免？
- CPU是如何调度任务的？如果你的任务对响应要求很高，你希望它总是能被先调度，该怎么办？

#### CPU如何读写数据？

一个CPU里通常会有多个CPU核心，并且每个CPU核心都有自己的L1 Cache和L2 Cache，而L1 Cache通常分为 **dCache（数据缓存）** 和 **iCache（指令缓存）** ，L3 Cache则是多个核心共享的，这就是CPU典型的缓存层次。

上面提到的都是CPU内部的Cache，放眼外部的话，还会有内存和硬盘，这些存储设备共同构成了金字塔存储层次。

cpu访问L1 cache的速度比访问内存快100倍，这就是为什么CPU里会有L1-L3 Cache的原因，目的就是把Cache作为CPU与内存之间的缓存层，以减少对内存的访问频率。

CPU从内存中读取数据到Cache的时候，并不是一个字节一个字节读取，而是一块一块的方式来读取数据的，这一块一块的数据被称为 **CPU Line（缓存行）** ，所以 **CPU Line 是CPU从内存读取数据到Cache的单位**。

至于CPU Line大小，在Linux中可以通过下述命令查到
``` shell
cat /sys/devices/system/cpu/cpu0/cache/index0/coherency_line_size 
```
那么对数组的加载，CPU就会加载数组里面连续的多个数据到Cache里，因此我们应该按照物理内存地址分布的顺序去访问元素，这样访问数组元素的时候，cache命中率就会很高，于是就能减少从内存读取数据的频率，从而提高程序的性能。

但是，在我们不使用数组，而是单独使用变量的时候，则会有Cache伪共享的问题，Cache伪共享问题上是一个性能杀手，我们应该回避它。

接下来，就来看看cache伪共享是什么？又如何避免这个问题？

现在假设有一个双核心的CPU，这两个cpu核心并行运行着两个不同的线程，它们同时从内存中读取两个不同的数据，分别是类型为`long`的变量A和B，这两个数据的地址在物理内存上是 **连续的**，如果cache line的大小是64字节，并且变量A在cache line的开头位置，那么这两个数据是位于 **同一个Cache Line**中，又因为CPU Line是CPU从内存读取数据到cache的单位，所以这两个数据会被同时读入到了两个CPU核心中各自cache中。

我们来思考一个问题，如果这两个不同核心的线程分别修改不同的数据，比如1号CPU核心的线程只修改了变量A，或2号CPu核心的线程只修改了变量b，会发生上面呢？

##### 分析伪共享的问题

现在我们结合保证多核缓存一致的MESI协议，来说明这一整个的过程。
1. 最开始变量A和B都还不在Cache里面，假设1号核心绑定了线程A，2号核心绑定了线程B，线程A只会读写变量A，线程B只会读写变量B
2. 1号核心读取变量A，由于CPU从内存读取数据到cache的单位是cache line，也正好变量A和变量B的数据归属于同一个Cache Line，所以A和B的数据都会加载到Cache，并将此Cache Line标记为 **独占** 状态。
3. 接着，2号核心开始从内存里读取变量B，同样的也是读取Cache Line大小的数据到cache中，此cache line中的数据也包含了变量A和变量B，此时1号和2号核心的cache line状态变为 **共享** 状态。
4. 1号核心需要修改变量A，发现此cache line的状态是 **共享** 状态，所以先需要通过总线发送消息给2号核心，通知2号核心把cache中对应的cache line 标记为 **已失效** 状态，然后1号核心对应的cache line 状态变成了 **已修改**状态，并且修改变量A。
5. 之后，2号核心需要修改变量B，此时2号核心的cache中对应的cache line 是已失效状态，另外由于1号核心的cache也有此相同的数据，且状态为 **已修改** 状态，所以要先把1号核心的cache对应的cache line写回到内存，然后2号核心再从内存读取cache line 大小的数据到cache中，最后把变量b修改到2号核心的cache张，并将状态标记为 **已修改** 状态。

所以，可以发现如果1号和2号cpu核心这样持续交替的分别修改变量A和B，就会重复4和5这两个步骤，cache并没有起到缓存的效果，虽然变量a和b之间其实并没有任何的关系，但是因为同时归属于一个cache line，这个cache line中的任意数据被修改后，都会相互影响，从而出现4和5这两个步骤。

因此，这种因为多个线程同时读写一个cache line的不同变量时，而导致cpu cache失效的现象称为 **伪共享（false sharing）** 。

##### 避免伪共享的方法

因此，对于多个线程共享的热点数据，即经常会修改的数据，应该避免这些数据刚好在同一个cache line中，否则就会出现为伪共享的问题。

接下来，看看在实际项目中是用什么方式来避免伪共享的问题的。

在Linux内核中存在 `_cacheline_aligned_in_smp`宏定义，是用于解决伪共享的问题。

``` c
#ifdef CONFIG_SMP
#define __cacheline_aligned_in_smp __cacheline_aligned
#else
#define __cacheline_aligned_in_smp
#endif
```

从上面的宏定义，可以看到：
- 如果在多核（MP）系统里，该宏定义是`__cacheline_aligned`，也就是Cache Line的大小；
- 而如果在单核系统里，该宏定义是空的；

因此，针对在同一个Cache Line中的共享的数据，如果在多核之间竞争比较严重，为了防止伪共享现象的发生，可以采用上面的宏定义使得变量在Cache Line里是对齐的。

举个例子：
``` c
struct test{
  int a;
  int b;
}
```

结构体里的两个成员变量a和b在物理内存上是连续的，于是它们可能会位于同一个cache line中

所以为了防止前面提到的cache伪共享问题，我们可以用上面介绍的宏定义，将b的地址设置为cache line对齐地址：
``` c
struct test{
  int a;
  int b __cacheline_aligned_in_smp;
}
```
这样a和b变量就不会在同一个cache line中了

我们再来看一个用用岑你干嘛的规避方案，有一个Java并发框架Disruptor使用 “字节填充+继承”的方式，来避免伪共享的问题。

Disruptor中有一个RingBuffer类经常被多个线程使用，代码如下：
```java
abstract class RingBufferPad{
  protected long p1,p2,p3,p4,p5,p6,p7;
}

abstract calss RingBufferFields<E> extends RingBufferPad{
  ......
  private final long indexMask;
  private final long Objejct[] entries;
  protected final int bufferSzie;
  protected final Sequencer sequencer;
  ......
}

public final class RingBuffer<E> extends RingBufferFields<E> implements Cursored,EventSequencer<E>,EventSink<E>{
  public static final long INITIAL_CURSOR_VALUE = Sequence.INITIAL_VALUE;
  protected long p1,p2,p3,p4,p5,p6,p7;
  ......
}
```

你可能会觉得RingBufferPad类里7个long类型的名字很奇怪，但事实上，它们虽然看起来毫无作用，但却对性能的提升起到了至关重要的作用。

我们都知道，CPU Cache从内存读取数据的单位是CPU Line，一般64位CPU的CPU Line的大小是64字节，一个long类型的数据是8个字节，所以CPU一下会加载8个long类型的数据。

根据JVM对象继承关系中父类成员和子类成员，内存地址是连续排列布局的，因此RingBufferPad中的7个long类型数据作为Cache Line **前置填充**，而RingBuffer中的7个long类型数据则作为Cache Line **后置填充**，这14个long变量没有任何实际用途，更不会对它们进行读写操作。

另外，RingBufferFelds里面定义的这些变量都是`final`修饰的，意味着第一次加载之后不会再修改，又 **由于前后各填充了7个不会被读写的long类型变量，所以无论怎么加载Cache Line，这整个Cache Line里都么有会发生更新操作的数据，于是只要数据被频繁地读取访问，就自然没有数据被换出Cache地可能，也因此不会产生伪共享地问题。**

#### CPU如何选择线程

了解完CPU读取数据的过程后，我们再来看看CPU是根据什么来选择当前要执行的线程。

在Linux内核中，进程和线程都是用`tark_struct`结构体表示的，区别在于线程地tark_struct结构体结构体里部分资源是共享了进程已创建地资源，比如内存地址空间、代码段、文件描述符等，所以Linux中的线程也被称为轻量级进程，因为线程的tark_struct相比进程的tark_struct承载的资源比较少，因此以轻得名。

一般来说，没有创建线程的进程，是只有单个执行流，它被称为主线程。如果想让进程处理更多的事情，可以创建多个线程分别去处理，但不管怎么样，它们对应到内赫里都是tark_struct。

所以，在Linux内核里的调度器，调度的对象就是tark_struct，接下来我们就把这个数据结构统称为 **任务**。

在Linux系统中，根据任务的优先级以及相应要求，主要分为两种，其中优先级的数值越小，优先级越高：
- 实时任务，对系统的响应时间要求很高，也就是要尽可能快的执行实时任务，优先级在`0-99`范围内的就算实时任务。
- 普通任务，响应时间没有很高的要求，优先级在`100-139`范围内都是普通任务级别；

##### 调度类

由于任务有优先级之分，Linux系统为了保障高优先级的任务能够尽可能早的被执行，于是分为了这几种调度类：
|调度类|调度器|调度策略|运行队列|
|-----|------|-------|-------|
|Deadline|Deadline 调度器|SCHED_DEADLINE|dl_rq|
|Realtime|RT 调度器|SCHED_FIFO <br> SCHED_RR|rt_rq|
|Fair|CFS 调度器|SCHED_NORMAL <br> SCHED_BATCH|cfs_rq|

Deadline和Realtime这两个调度类，都是应用于实时任务的，这两个调度类的调度策略合起来共有三种，它们的作用如下：
- SCHED_DEADLINE：是按照dealine进行调度的，距离当前时间点最近的deadline的任务就会被优先调度。
- SCHED_FIFO：对于相同优先级的任务，按先来先服务的原则，但是优先级更高的任务，可以抢占低优先级的任务，也就是优先级高的可以“插队”；
- SCHED_RR：对于相同优先级的任务，轮流着运行，每个任务都有一定的时间片，当用完时间片的任务会被放到队列尾部，以保证相同优先级任务的公平性，但是高优先级的任务依然可以抢占低优先级的任务；

而Fair调度类是应用于普通任务，都是由CFS调度器管理的，分为两种调度策略：
- SCHED_NORMAL：普通任务使用的调度策略；
- SCHED_BATCH：后台任务的调度策略，不和终端进行交互，因此在不影响其他需要交互的任务，可以适当降低它的优先级。

##### 完全公平调度

我们平日里遇到的基本都是普通任务，对于普通任务来说，公平性最重要，在Linux里面，实现一个基于CFS的调度算法，也就是 **完全公平调度（Completely Fair Scheduling）**。

这个算法的理念是想让分配给每个任务的CPU时间是一样，于是它为每个任务安排一个虚拟运行时间vruntime，如果一个任务在运行，其运行的越久，该任务的vruntime自然就会越大，而没有被运行的任务，vruntime是不会变化的。

那么，**在cfs算法调度的时候，会优先选择vruntime少的任务**，以保证每个任务的公平性。

这就好比，让你把一桶的奶茶平均分到10杯奶茶杯里，你看着哪杯奶茶少，就多倒一些；哪个多了，就先不倒，这样经过多轮操作，虽然不能保证每杯奶茶完全一样多，但至少是公平的。

当然，上面提到的例子没有考虑到优先级的问题，虽然是普通任务，但是普通任务之间还是有优先级区分的，所以在计算虚拟运行时间vruntime还要考虑普通任务的 **权重值**，注意权重值并不是优先级的值，内核中会有一个nice级别与权重值的转换表，nice级别越低的权重值就越大，至于nice值是什么，后面就会提到。

`虚拟运行时间vruntime += 实际运行时间delta_exec * NICE_0_LOAD / 权重`

可以不用管NICE_0_LOAD 是什么，你就认为他是一个常量，那么在同样的实际运行时间里，高权重任务的vruntime比低权重任务的vruntime **少** ，你可能会奇怪为什么是少的？你还记得CFS调度吗，它是会优先选择vruntime少的任务进行调度，所以高权重的任务就会被优先调度了，于是高权重的获得的实际运行时间自然就多了。

##### CPU运行队列

一个系统通常会运行着很多任务，多任务的数量基本都是远超CPU核心数量，因此这时候就需要 **排队**。

事实上，每个CPU都有自己的 **运行队列（Run Queue，rq）** ，用于描述在此CPU上所运行的所有进程，其队列包含三个运行队列，Deadline运行队列dl_rq、实时任务运行队列rt_rq和CFS运行队列csf_rq,其中csf_rq是用红黑树来描述的，按vruntime大小来排序的，最左侧的叶子节点，就是下次会被调度的任务。

这几种调度类是有优先级的，优先级如下：Deadline>Realtime>Fair，这意味着Linux选择下一个任务执行的时候，会按照此优先级顺序进行选择，也就是说先从 `dl_rq`里选择任务，然后从 `rt_rq`里选择任务，最后从`csf_rq`里选择任务。因此，**实时任务总是会比普通任务优先被执行**。

##### 调度优先级

如果我们启动任务的时候，没有特意去指定优先级的话，默认情况下都是普通任务，普通任务的调度类是Fail，由CFS调度器来进行管理。CFS调度器的目的是实现任务运行的公平性，也就是保障每个任务的运行的时间是差不多的。

如果你想让某个普通任务有更多的执行时间，可以调整任务的 `nice`值，从而让优先级高一些的任务执行更多时间。nice的值能设置的范围是`-20~19`，值越低，表明优先级越高，因此-20是最高优先级，19则是最低优先级，默认优先级是0。

是不是觉得nice值的范围很诡异？事实上，nice值并不是表示优先级，而是表示优先级的修正数值，它与优先级（priority）的关系是这样的：priority(new)=priority(old)+nice。内核中，priority的范围是0~139，值越低，优先级越高，其中前面的0~99范围是提供给实时任务使用的，而nice值是映射到100~139，这个范围提供给普通任务用的，因此nice值调整的是普通任务的优先级。

在前面我们提到了，权重值与nice值的关系，nice值越低，权重值越大，计算出来的vruntime就会越少，由于cfs算法调度时，就会优先选择vruntime少的任务进行执行，所以nice值越低，任务的优先级就越高。

我们可以在启动任务的时候，可以指定nice的值，比如将mysqld以-3优先级
```shell
nice -n -3 /usr/sbin/mysqld
```

如果想修改已经运行中的任务的优先级，则可以使用`renice`来调整nice值:
```shell
renice -10 -p <进程pid>
```

nice调整的是普通任务的优先级，所以不管怎么缩小nice值，任务永远是普通任务，如果某些任务要求实时性比较高，那么可以调考虑改变任务的调度策略，变成实时任务

```shell
chrt -f 1 -p 1996
```

#### 总结

理解cpu是如何读写数据的前提，是要理解CPU的架构，CPU内部的多个Cache+外部的内存和磁盘都构成了金字塔的存储器结构，在这个金字塔中，越往下，存储器的容量就越大，但访问速度就会小。

CPU读写数据的时候，并不是按一个一个字节为单位来进行读写，而是以CPU Line大小为单位，CPu Line大小一般是64字节，也就意味着CPU读写数据的时候，每一次都是以64字节大小为一块进行操作。

因此，如果我们操作的数据是数组，那么访问数组元素的时候按内存分布的地址顺序进行访问，这样能充分利用到Cache，程序的性能得到提升。但如果操作的数据不是数组，而是普通的变量，并在多核CPU的情况下，我们还需要避免Cache Line伪共享的问题。

所谓的 Cache Line 伪共享问题就是，多个线程同时读写同一个 Cache Line 的不同变量时，而导致 CPUCache 失效的现象。那么对于多个线程共享的热点数据，即经常会修改的数据，应该避免这些数据刚好在同一个 Cache Line 中，避免的方式一般有 Cache Line 大小字节对⻬，以及字节填充等方法。

系统中需要运行的多线程数一般都会大于 CPU 核心，这样就会导致线程排队等待 CPU，这可能会产生一定的延时，如果我们的任务对延时容忍度很低，则可以通过一些人为手段干预 Linux 的默认调度策略和优先级。

### 1.6 软中断

#### 中断是什么？

**什么是中断** ：中断是系统用来响应硬件设备请求的一种机制，操作系统收到硬件的中断请求，会打断正在执行的进程，然后调用内核中的中断处理程序来响应请求。

举个例子：
```
点外卖时，平台上会显示配送进度，但是不能一直傻傻盯着，要去干别的事情，等外卖员通过 “电话”通知我，电话响了，就会停下手中的事情，去拿外卖。
```

这里的打电话，就是计算机的中断。

从这个例子，我们可以知道，中断是一种异步的事件处理机制，可以提高系统的并发处理能力。

操作系统收到了中断请求，会打断其他进程的运行，所以 **中断请求的响应程序，也就是中断处理程序，要尽可能快的执行完，这样可以减少对正常进程运行调度地影响**。

而且，中断程序在响应中断时，可能还会“临时中断关闭”，意味着当前中断程序没有执行完之前，系统中其他的中断请求都无法响应，也就是说中断有可能会丢失，所以中断处理程序要短且快。

回到外卖的例子，如果点了两份外卖，第一份外卖到的时候，配送员打了很长时间的电话，第二个配送员在打电话时候就无法打通电话，可能尝试几次就走掉了。

#### 什么是软中断

Linux系统 **为了解决中断处理程序执行过长和中断丢失的问题，将中断过程分成了两个阶段，分别是上半部和下半部分**。

- **上半部用来快速处理中断**，一般会暂时关闭中断请求，主要负责处理跟硬件紧密相关或者时间敏感的事情。
- **下半部用来延迟处理上半部未完成的工作**，一般以“内核线程”的方式来运行。

依旧是外卖：第一个配送员给我长时间通电话，导致第二位配送员无法拨通我的电话，其实当接到第一个电话，可以告诉配送员下楼见面在手，然后挂断电话，拿到外卖后再说其他事情。

这样，第一位配送员不会占用太多时间，第二位配送员可以拨通我的电话。

再举一个计算机的例子：

1. 网卡收到网络包后，通过 **硬件中断**通知内核有新的数据到了，于是内核会调用对应的中断处理程序来响应事件，这个事件的处理也是会分成上半部和下半部。
2. 上部分要做到快速处理，所以只要把网卡的数据放到内存中，然后更新硬件寄存器的状态，比如把状态更新为表示数据已经读到内存中的状态值。
3. 接着，内核会触发一个 **软中断**，把一些处理比较耗时且复杂的事情，交给“软中断处理程序”去做，也就是中断的下半部，其主要是需要从内存中找到网络数据，再按照网络协议栈，对网络数据进行逐层解析和处理，最后把数据送给应用程序。

所以中断程序的上部分和下半部可以理解为：
- **上半部直接处理硬件请求，也就是硬中断**，主要是负责耗时短的工作，特点是快速执行；
- **下半部是由内核触发，也就是软中断**，主要是负责上半部未完成的工作，通常是耗时比较长的事情，特点是延迟执行。

其他区别：
硬中断（上半部）会打断CPU正在执行的任务，然后立即执行中断处理程序；而软中断（下半部）是以内核线程的方式执行，并且每一个CPU都对应一个软中断内核线程，名字通常为"ksoftirqd/0"

不过，软中断不只是包括硬件设备中断处理程序的下半部，一些内核自定义事件也属于软中断，比如内核调度等、RCU锁（内核里常用的一种锁）等。

##### 系统里有哪些软中断

在Linux系统里，可以通过 `/proc/softirqs`的内容来知晓"软中断"的运行情况，以及`/proc/interrupts`的内容来知晓"硬中断"的运行情况。

每一个cpu都有自己对应的不同类型软中断的 **累计运行次数**。

累计中断次数没有参考意义，但是系统的 **中断次数的变化速率**才是要关注的
可以使用`watch -d cat /proc/softirqs`命令查看中断次数的变化速率。

##### 如何定位软中断cpu使用率过高的问题

要想知道当前的系统的软中断情况，可以使用`top`命令查看

如果要查看那张软中断类型导致的，可以使用`watch -d cat/proc/softirqs`命令查看每个软件中断类型的中断次数的变化速率。

一般对于网络I/O比较高的Web服务器，`NET_RX`网络接收中断的变化速率相比其他中断类型快很多。

如果发现`NET_RX`网络接收中断次数的变化速率过快，接下里就可以使用 `sar -n DEV`查看网卡的网络包接受速率情况，然后分析是哪个网卡有大量的网络包进来。

接在再通过 `tcpdump`抓包，分析这些包的来源，如果是非法的地址，可以考虑加防火墙，如果是正常浏览，则要考虑硬件升级。

#### 总结

为了避免由于中断处理程序执行时间过⻓，而影响正常进程的调度，Linux 将中断处理程序分为上半部和下半部：
- 上半部，对应硬中断，由硬件触发中断，用来快速处理中断；
- 下半部，对应软中断，由内核触发中断，用来异步处理上半部未完成的工作；

Linux 中的软中断包括网络收发、定时、调度、RCU 锁等各种类型，可以通过查看 /proc/softirqs 来观察软中断的累计中断次数情况，如果要实时查看中断次数的变化率，可以使用 watch -d cat /proc/softirqs 命令。

每一个 CPU 都有各自的软中断内核线程，我们还可以用 ps 命令来查看内核线程，一般名字在中括号里面的，都认为是内核线程。

如果在 top 命令发现，CPU 在软中断上的使用率比较高，而且 CPU 使用率最高的进程也是软中断ksoftirqd 的时候，这种一般可以认为系统的开销被软中断占据了。

这时我们就可以分析是哪种软中断类型导致的，一般来说都是因为网络接收软中断导致的，如果是的话，可以用 sar 命令查看是哪个网卡的有大量的网络包接收，再用 tcpdump 抓网络包，做进一步分析该网络包的源头是不是非法地址，如果是就需要考虑防火墙增加规则，如果不是，则考虑硬件升级等。